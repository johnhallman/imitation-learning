{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf \n",
    "from keras import Model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.optimizers import Adam, Adagrad, Adadelta\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize and render policy\n",
    "def visualize(env, pi_opt, pi_expert):\n",
    "    data = []\n",
    "    obs = env.reset()\n",
    "    for t in range(1000):\n",
    "        data.append(obs)\n",
    "        env.render()\n",
    "        time.sleep(0.03) # slows down process to make it more visible\n",
    "        action = pi_opt.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Model ran {} time steps\".format(t+1))\n",
    "            break\n",
    "\n",
    "    preds = np.array([pi_opt.predict(obs)[0] for obs in data])\n",
    "    correct = np.array([pi_expert.predict(obs)[0] for obs in data])\n",
    "    print(\"MSE: {}\".format(((preds - correct)**2).mean()))\n",
    "\n",
    "# policy class\n",
    "class Policy(Model):\n",
    "    def __init__(self, model):\n",
    "        super(Policy, self).__init__(name='policy')\n",
    "        self.model = model\n",
    "\n",
    "    @classmethod\n",
    "    def random(cls, state_dim, action_dim):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(max(state_dim, action_dim), input_dim=state_dim, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(action_dim, kernel_initializer='normal', activation='relu'))\n",
    "        return cls(model)\n",
    "    \n",
    "    def predict(self, input):\n",
    "        return self.model.predict(np.expand_dims(input, axis=0))[0]\n",
    "    \n",
    "# expert policy\n",
    "class Expert(Policy):\n",
    "    def __init__(self, filename, state_dim, action_dim):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(16, input_dim=state_dim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(16))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(16))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(action_dim))\n",
    "        model.add(Activation('linear'))\n",
    "        model.load_weights(filename)\n",
    "        Policy.__init__(self, model)\n",
    "\n",
    "# takes a policy and estimates its discounted distribution\n",
    "def policy_distribution(env, policy, N, gamma):\n",
    "    trajects = []\n",
    "    weights = []\n",
    "    for i in range(N):\n",
    "        obs = env.reset()\n",
    "        for t in range(1000):\n",
    "            trajects.append(obs)\n",
    "            action = policy.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            if done: \n",
    "                const = (1 - gamma) / (1 - gamma**(t+2))\n",
    "                weights += [const * gamma**k for k in range(t+1)]\n",
    "                break\n",
    "    spread = np.std(trajects) * np.power((4.0 / 3 * len(trajects)), 1.0/5)\n",
    "    est = KernelDensity(bandwidth=spread)\n",
    "    est.fit(np.array(trajects), sample_weight=np.array(weights))\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize environment\n",
    "def plot_env(env_name=\"Pendulum-v0\"):\n",
    "    env = gym.make(env_name)\n",
    "    state_dim, action_dim = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "    print(\"State, action dimension: {}, {}\".format(state_dim, action_dim))\n",
    "    policy = Policy.random(state_dim, action_dim)\n",
    "    expert = Expert(\"models/pendulum_expert.h5f\", state_dim, action_dim)\n",
    "    p_dist = policy_distribution(env, policy, 10, 0.95)\n",
    "    e_dist = policy_distribution(env, expert, 10, 0.95)\n",
    "    visualize(env, policy, expert)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State, action dimension: 3, 1\n",
      "Model ran 200 time steps\n",
      "MSE: 11.663902282714844\n"
     ]
    }
   ],
   "source": [
    "#plot_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4484348772720065\n",
      "[0.44843488]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "x = np.array([[1, 2, 3], [2, 3, 4], [1.5, 2.5, 3.5], [1, 2, 2], [0, 2, 4]])\n",
    "w = np.array([1, 1, 1, 1, 1])\n",
    "kernel = stats.gaussian_kde(x.T, weights=w)\n",
    "print(kernel.pdf(np.array([1, 2, 3]))[0])\n",
    "print(kernel.evaluate(np.array([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
